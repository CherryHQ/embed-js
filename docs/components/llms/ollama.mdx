---
title: 'Ollama'
---

You can also use locally running Ollama models. Installation instructions for Ollama can be found [here](https://ollama.com/).
Once Ollama is installed, you can start a local LLM by executing `ollama run <modelname>`.

## Install Ollama addon

```bash
npm install @cherrystudio/embedjs-ollama
```

## Usage

```ts
import { RAGApplicationBuilder } from '@cherrystudio/embedjs';
import { Ollama } from '@cherrystudio/embedjs-ollama';

const app = await new RAGApplicationBuilder()
.setModel(new Ollama({
    modelName: "llama3",
    baseUrl: 'http://localhost:11434'
}))
```
